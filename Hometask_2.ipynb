{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Школа машинного обучения\n",
    "\n",
    "### Физтех-Школа Прикладной математики и информатики МФТИ \n",
    "### Лаборатория нейронных сетей и глубокого обучения (DeepHackLab)  \n",
    "дедлайн: 4 апреля 23: 59 (MSK)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание 2\n",
    "### Метод k-ближайших соседей \n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neighbors import KNeighborsClassifier, KDTree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "import scipy.stats as sps\n",
    "import sklearn\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.collections \n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вопрос:** Почему важно, чтобы в тестовой и обучающей выборке пропорции классов были максимально похожи? В качестве примера рассмотрите Ирисы Фишера: в нём три класса, каждый занимает по трети датасета. Далее мы разделяем выборку в пропорциях 2:1, например, при кросс-валидации с тремя фолдами. Что может пойти не так?\n",
    "\n",
    "В этом ноутбуке за равенство классов отвечает StratifiedKFold (см. пример ниже, подробнее: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При недостаточном количестве данных можно получить смещение. Класс с меньшей выборкой оказывает меньшее влияние на классификатор. Например: вы решили вылечить рак и у вас будет датасет в котором большинство людей не болеют раком, \n",
    "соответственно люди с раком (2% от всего населния мира) будет иметь очень маленькое влияние на модель."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 1** Примените kNN к классическому <a href=\"https://ru.wikipedia.org/wiki/%D0%98%D1%80%D0%B8%D1%81%D1%8B_%D0%A4%D0%B8%D1%88%D0%B5%D1%80%D0%B0\">набору данных \"Ирисы Фишера\" </a>. Подберите оптимальное число k c помощью поиска по сетке и кросс-валидации. Постройте график зависимости качества от k. Используйте метрику accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "\n",
    "x = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Поиск по сетке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вы уже умеете пользоваться GridSearchCV (см семинар по knn)\n",
    "\n",
    "normal_model = KNeighborsClassifier()\n",
    "interesting_model = KNeighborsClassifier()\n",
    "converted_model = KNeighborsClassifier()\n",
    "\n",
    "params = {'n_neighbors': np.arange(1, 50) }\n",
    "\n",
    "gscv = GridSearchCV(normal_model, params, cv=5)\n",
    "gscv.fit(x, y)\n",
    "print('Normal ', gscv.best_params_, gscv.best_score_)\n",
    "\n",
    "X_new = x * np.array((100, 1, 1, 1))\n",
    "interesting_gscv = GridSearchCV(interesting_model, params, cv=5)\n",
    "interesting_gscv.fit(X_new, y)\n",
    "print('Changed ', interesting_gscv.best_params_, interesting_gscv.best_score_)\n",
    "\n",
    "\n",
    "X_converted = x * np.array((100, 100, 100, 100))\n",
    "converted_gscv = GridSearchCV(converted_model, params, cv=5)\n",
    "converted_gscv.fit(X_converted, y)\n",
    "print('Converted ', converted_gscv.best_params_, converted_gscv.best_score_)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.grid(linestyle=':', linewidth=2)\n",
    "plt.plot(params['n_neighbors'], gscv.cv_results_['mean_test_score'], 'ro-', label='normal')\n",
    "plt.plot(params['n_neighbors'], interesting_gscv.cv_results_['mean_test_score'], 'go-', label='changed')\n",
    "plt.plot(params['n_neighbors'], converted_gscv.cv_results_['mean_test_score'], 'bo-', label='converted')\n",
    "plt.xlabel('k', fontsize=20)\n",
    "plt.ylabel('accuracy', fontsize=20)\n",
    "plt.title('knn',  fontsize=20)\n",
    "plt.legend(fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если вы смотрели в данные, то вы видели, что признаки примерно одного порядка, т.к. это длины и ширины в сантиметрах. Предположим теперь, что один из признаков измерялся в десятых долях миллиметра. Точно так же подберите оптимальное k для новых данных, сравните качество и постройте график. Что нужно делать, чтобы такая проблема не возникала?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ответ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужно приводить все величины в одну систему измерения (на графике видно, что если все величины в измеряются в одной единицей, то отклонений в измерениях почти нет). Ещё было бы неплохо иметь какие-то дополнительные измерения в других единицах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 2:** Реализуйте kNN. Cравните скорости работы реализации с distance_slow,  distance_fast c реализацией из sklearn. Проверьте, что качество такое же. \n",
    "Считать, что интерфейс fit и predict такой же, как у KNeighborsClassifier из sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_slow(v, a_list):\n",
    "    \"\"\"\n",
    "    Функция, по вектору v и спиcку векторов a\n",
    "    находящая попарные расстояния v <-> a[i]\n",
    "    и возвращающая их как numpy.ndarray той же длины,\n",
    "    что и список a\n",
    "    (Работает медленно)\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for i in range(a_list.shape[0]):\n",
    "        length = 0\n",
    "        for j in range(a_list.shape[1]):\n",
    "            length += (v[j] - a_list[i, j]) ** 2\n",
    "        result.append(length)\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def distance_fast(v, a_list):\n",
    "    \"\"\"\n",
    "    Аналог distance_slow. Использует numpy, работает быстро.\n",
    "    \"\"\"\n",
    "    # return < .. Достаточно одной строчки. Считатайте, что v, a_list - numpy.ndarray подходящего размера ..\n",
    "    # return ((a_list - v) ** 2).sum()\n",
    "    # return list(map(lambda x: np.linalg.norm(v - x) ** 2, a_list))\n",
    "    return np.sum((v - a_list) ** 2, axis = 1)\n",
    "    \n",
    "# v = np.ndarray(shape = 1, buffer = np.array([1 , 2]))\n",
    "# a_list = np.ndarray(shape = 2, buffer = np.array([[1, 2], [3, 4]]))\n",
    "\n",
    "a_list = np.array([[1, 2], [3, 4]])\n",
    "v = np.array([1, 2])\n",
    "\n",
    "print(distance_fast(v, a_list))\n",
    "print(distance_slow(v, a_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class kNNClassifier:\n",
    "    def __init__(self, k=3, distance = distance_slow):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        k: int\n",
    "            Число соседей\n",
    "\n",
    "        distance: *alias\n",
    "            функция, по вектору v и спиcку векторов a\n",
    "            находящая попарные расстояния v <-> a[i]\n",
    "            и возвращающая их как numpy.ndarray той же длины,\n",
    "            что и список a\n",
    "        \"\"\"\n",
    "        self._k = k\n",
    "        self._distance = distance\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        self._X = np.copy(X_train)  # Копируем данных, чтобы они не перезаписывались извне\n",
    "        self._y = np.copy(y_train)\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        X_test = np.array(X_test)\n",
    "        predictions = []\n",
    "        # print(X_test)\n",
    "        # objects_count = < .. число объектов, которые нужно классифицировать .. >\n",
    "        \n",
    "        objects_count = X_test.shape[0]\n",
    "        \n",
    "        # print(X_test.shape[0])\n",
    "        # print(X_test.shape[1])\n",
    "        # print(X_test[0])\n",
    "        # print(len(X_test))\n",
    "        for i in range(objects_count):\n",
    "            pairwise_distances = self._distance(X_test[i], self._X) # < .. вызовите self._distance .. >\n",
    "            \n",
    "            k_nearest = self._y[np.argsort(pairwise_distances)[:self._k]]  # нашли k ближайших.\n",
    "            \n",
    "            \n",
    "            '''\n",
    "            unique_values, counts = < .. используйте numpy.unique с return_counts=True, \n",
    "                                        чтобы найти какие классы есть среди k соседей \n",
    "                                        и сколько раз каждый из них встречается .. >\n",
    "            '''\n",
    "            unique_values, counts = np.unique(k_nearest, return_counts=True)      \n",
    "            # print('----->', unique_values, counts, k_nearest)\n",
    "            # Если вы что-то не понимаете - у numpy замечательная документация       \n",
    "                \n",
    "            prediction = unique_values[np.argmax(counts)]\n",
    "            # Предсказываем класс, представителей которого больше всего\n",
    "            \n",
    "            predictions.append(prediction) \n",
    "            \n",
    "        return predictions\n",
    "    \n",
    "    def get_params(self, deep=False):\n",
    "        \"\"\"\n",
    "        Функция, необходимая для работы GridSearchCV\n",
    "        Возвращает параметры данного экземпляра класса\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"k\": self._k,\n",
    "            \"distance\": self._distance\n",
    "        }\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        \"\"\"\n",
    "        Функция, необходимая для работы GridSearchCV\n",
    "        Устанавливает параметры из params \n",
    "        (В данном случае пересоздаёт экземпляр класса \n",
    "        и возвращает его)\n",
    "        \"\"\"\n",
    "        self.__init__(**params)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()\n",
    "\n",
    "\n",
    "x = data.data\n",
    "y = data.target\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "print(sklearn.model_selection.cross_val_score(clf, x, y, cv=StratifiedKFold(shuffle = False), scoring=\"accuracy\", ))\n",
    "\n",
    "clf = kNNClassifier(k = 3)\n",
    "print(sklearn.model_selection.cross_val_score(clf, x, y, cv=StratifiedKFold(shuffle = False), scoring=\"accuracy\", ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf = kNNClassifier()\n",
    "\n",
    "params = {\n",
    "    \"k\":[1, 3, 5, 7, 9, 11, 13, 15, 17],\n",
    "    \"distance\":[distance_slow]\n",
    "}\n",
    "gscv = GridSearchCV(clf, params, cv=StratifiedKFold(shuffle = False, n_splits=5), scoring=\"accuracy\", )\n",
    "gscv.fit(x, y)\n",
    "print(\"Best params: {}. Best score: {}\".format(gscv.best_params_, gscv.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "clf = kNNClassifier()\n",
    "\n",
    "params = {\n",
    "    \"k\":[1, 3, 5, 7, 9, 11, 13, 15, 17],\n",
    "    \"distance\":[distance_fast]\n",
    "}\n",
    "gscv = GridSearchCV(clf, params, cv=StratifiedKFold(shuffle = False, n_splits=5), scoring=\"accuracy\", )\n",
    "gscv.fit(x, y)\n",
    "print(\"Best params: {}. Best score: {}\".format(gscv.best_params_, gscv.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "params = {\n",
    "    \"n_neighbors\":[1, 3, 5, 7, 9, 11, 13, 15, 17],\n",
    "}\n",
    "gscv = GridSearchCV(clf, params, cv=StratifiedKFold(shuffle = False, n_splits=5), scoring=\"accuracy\", )\n",
    "gscv.fit(x, y)\n",
    "print(\"Best params: {}. Best score: {}\".format(gscv.best_params_, gscv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf = KNeighborsClassifier(n_neighbors=3, algorithm='brute')\n",
    "\n",
    "params = {\n",
    "    \"n_neighbors\":[1, 3, 5, 7, 9, 11, 13, 15, 17],\n",
    "}\n",
    "gscv = GridSearchCV(clf, params, cv=StratifiedKFold(shuffle = False, n_splits=5), scoring=\"accuracy\", )\n",
    "gscv.fit(x, y)\n",
    "print(\"Best params: {}. Best score: {}\".format(gscv.best_params_, gscv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf = KNeighborsClassifier(n_neighbors=3, algorithm='kd_tree')\n",
    "\n",
    "params = {\n",
    "    \"n_neighbors\":[1, 3, 5, 7, 9, 11, 13, 15, 17],\n",
    "}\n",
    "gscv = GridSearchCV(clf, params, cv=StratifiedKFold(shuffle = False, n_splits=5), scoring=\"accuracy\", )\n",
    "gscv.fit(x, y)\n",
    "print(\"Best params: {}. Best score: {}\".format(gscv.best_params_, gscv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вывод по задаче:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Медленнее всего работает самописный knn с **distance_slow** ~=3.33 sec на ``GridSearchCV``.\n",
    "Сапописный knn с **distance_fast** тратит в ~= 6.85 раз меньше времени (490 ms).\n",
    "\n",
    "knn из ``sklearn`` работает быстрее самописного knn\n",
    "\n",
    "Скорость работы ``sklearn.KNeighborsClassifier``\n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "        <td>\n",
    "            <h2> Auto </h2>\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2> Brute </h2>\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2> KDTree </h2>\n",
    "        </td>\n",
    "    </tr>\n",
    "    \n",
    "    <tr>\n",
    "        <td>\n",
    "            <h2>  112 ms </h2>\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2>  150 ms </h2>\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2> 140 ms </h2>\n",
    "        </td>\n",
    "     </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "__Из таблицы видно, что самый тривиальный способ - самый медленный. Но чтобы получить более точные результаты по алгоритмам, нужно иметь выборку больше, чем в данной задаче.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Комментарий:** Интересующиеся могут изучить <a href=\"https://habrahabr.ru/post/312882/\">kd-tree</a>, позволяющее рассматривать меньшее число расстояний. Так же эта структура используется для отрисовки компьютерной графики."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача 3:** Пусть в данных предыдущей задачи мы получили измерения только двух признаков. Тогда признаки одного обьекта можно представить как точку на плоскости, которой в соответствие поставлен некоторый класс (можно визуализировать это как цвет). Постройте графики, изображающие принадлежность всех точек плоскости к классам для различных k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()\n",
    "X = data.data[:, [False, True, True, False]]\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cmap = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "plt.figure(figsize=(14, 14))\n",
    "for i, k in enumerate([1, 3, 5, 7]):\n",
    "    plt.subplot(221 + i)\n",
    "\n",
    "    h = 400\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.linspace(X[:, 0].min(), X[:, 0].max(), h),\n",
    "        np.linspace(X[:, 1].min(), X[:, 1].max(), h)\n",
    "    )\n",
    "    \n",
    "    X_grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "        \n",
    "    # Обучите (k)NN на данных X_train, y_train\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    Z  = knn.predict(X_grid)\n",
    "    z = knn.predict(X_test)\n",
    "    # Z = < .. предсказание на X_grid ..>\n",
    "    \n",
    "    acc = accuracy_score(y_test, z)\n",
    "    \n",
    "   \n",
    "   \n",
    "   \n",
    "    # acc = < .. вычислите качество на y_test, X_test. Используйте функцию accuracy_score .. >\n",
    "    # http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.title(\"k = {}, accuracy = {}\".format(k, round(acc, 3)))\n",
    "    zz = np.array(Z).reshape(xx.shape)\n",
    "   \n",
    "    # Вызовите plt.pcolormesh для точек xx, yy, zz и цветовой схемы cmap\n",
    "    # https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.pcolormesh.html\n",
    "    \n",
    "    plt.pcolormesh(xx, yy, zz, cmap=cmap)\n",
    "    \n",
    "    plt.xlabel(data[\"feature_names\"][1])\n",
    "    plt.ylabel(data[\"feature_names\"][2])\n",
    "    for i in range(3):\n",
    "        plt.scatter(X[y == i, 0], X[y == i, 1], color=[\"red\", \"green\", \"blue\"][i], label=data[\"target_names\"][i])\n",
    "    plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как меняется форма разделяющей кривой при увеличении k и как это сказывается на тестовом качестве? Не забывайте, что иногда в данных встречаются недостоверные измерения, вызванные множеством факторов, например, проблемами при переводе данных из одного формата в другой, в том числе при занесении непосредственных измерений в компьютер."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ответ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При увеличении k разделяющая кривая начинает выравниваться по x и это плохо влияет на тестовое качество модели, из-за того, что кривая выровнилась, то точки, которые лежали рядом, могут попасть из области одного цвета в другую"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 4*:** Предлагается датасет, состоящий из писем на две тематики. Задача - научиться классифицировать письма по темам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    'rec.autos',  # тем больше, чем две. Попробуйте другие.)\n",
    "    'sci.space',\n",
    "]\n",
    "\n",
    "dataset = fetch_20newsgroups(subset='all', categories=categories,\n",
    "                             shuffle=True, random_state=42)\n",
    "\n",
    "X = dataset.data\n",
    "y = dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X), len(y), y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим два письма из выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(X[-2])  # Класс 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[2])  # Класс 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В целом, после прочтения понятно, что первое письмо про космос а второе - про машины. Для того, чтобы классифицировать тексты, нужно перевести их в удобный для алгоритма вид, т.е. сделать из письма вектор. \n",
    "Прделагается делать это так: составить список всех используемых слов. Зафиксировать число N самых популярных слов, которые мы будет рассматривать. Каждому письму сопоставлять вектор длины N следующего вида: в a[i] записано число вхождений i-го по популярности слова. Данную задачу решает CountVectoizer: используя его преобразуйте тексты в векторы и подсчитайте качество (accuracy) на классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(max_features=50)\n",
    "vect.fit(X)\n",
    "Xt = vect.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверьте качество kNN на данных Xt, y.  Метрика - accuracy\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(Xt, y, test_size=0.33, random_state = 42)\n",
    "knn.fit(x_train, y_train)\n",
    "\n",
    "result = knn.predict(x_test)\n",
    "\n",
    "accuracy_score(y_test, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очевидна проблема: самые часто встречающиеся слова встречаются одинаково часто во всех текстах: это a, the, и прочие.\n",
    "Для этой проблемы также существует стандартное решение: Проверьте качество теперь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создайте экземпляр CountVectorizer с параметрами max_features=50, stop_words=\"english\"\n",
    "# Как в примере выше, преобразуйте X в Xt\n",
    "cnt = CountVectorizer(max_features=50, stop_words=\"english\")\n",
    "\n",
    "cnt.fit(X)\n",
    "Xt = cnt.transform(X)\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(Xt, y, test_size=0.33, random_state = 42)\n",
    "knn.fit(x_train, y_train)\n",
    "\n",
    "result = knn.predict(x_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверьте качество kNN на данных Xt, y.  Метрика - accuracy\n",
    "accuracy_score(y_test, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нетрудно догадаться, какие слова будут самыми популярными:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect.transform([\"cake space space car car car\"]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество метрических алгоритмов очень сильно зависит от метрики (функции расстояния). Сравните качество метрик \"minkowski\" и \"cosine\" - последняя считает величину, численно равную единице минут косинус угла между векторами, что более уместно для текстов, что обычная евклидова метрика (metric=\"minkowski\", p=2). Найдите параметры, на которых достигается лучшее качество."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"n_neighbors\": range(1, 15, 2), \n",
    "    \"metric\": [\"minkowski\", \"cosine\"]\n",
    "}\n",
    "\n",
    "# Вы знаете, что делать.)\n",
    "\n",
    "gs = GridSearchCV(knn, params, cv=5)\n",
    "gs.fit(Xt, y)\n",
    "gs.best_params_, gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При большом числе признаков метрические алгоритмы обычно плохо работают, подробнее: <a href=\"https://en.wikipedia.org/wiki/Curse_of_dimensionality\">проклятие размерности </a>."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
